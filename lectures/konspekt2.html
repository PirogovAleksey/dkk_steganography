<!DOCTYPE html>
<html lang="uk">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#0ea5e9">
  <title>Математичні основи стеганографії — Конспект | Стеганографія</title>
  <link rel="icon" type="image/svg+xml" href="../img/favicon.svg">
  <link href="https://fonts.googleapis.com/css2?family=Plus+Jakarta+Sans:wght@400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../css/style.css">
  <script>if(localStorage.getItem('theme')==='dark'){document.documentElement.classList.add('dark');}</script>
</head>
<body>

  <aside>
    <div class="logo">
      <div class="logo-icon">
        <svg viewBox="0 0 24 24">
          <path d="M22 10v6M2 10l10-5 10 5-10 5z"/>
          <path d="M6 12v5c3 3 9 3 12 0v-5"/>
        </svg>
      </div>
      <div class="logo-text">
        Стеганографія
        <span>онлайн-курс</span>
      </div>
    </div>
    <nav aria-label="Головна навігація">
      <a href="../index.html" class="active">
        <span class="nav-icon" aria-hidden="true"><svg viewBox="0 0 24 24"><path d="M4 19.5v-15A2.5 2.5 0 0 1 6.5 2H20v20H6.5a2.5 2.5 0 0 1 0-5H20"/></svg></span>
        Лекції
      </a>
      <a href="../practicals.html">
        <span class="nav-icon" aria-hidden="true"><svg viewBox="0 0 24 24"><rect x="2" y="3" width="20" height="14" rx="2"/><path d="M8 21h8"/><path d="M12 17v4"/></svg></span>
        Практичні
      </a>
      <a href="../tests.html">
        <span class="nav-icon" aria-hidden="true"><svg viewBox="0 0 24 24"><path d="M9 11l3 3L22 4"/><path d="M21 12v7a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V5a2 2 0 0 1 2-2h11"/></svg></span>
        Тести
      </a>
      <a href="../materials.html">
        <span class="nav-icon" aria-hidden="true"><svg viewBox="0 0 24 24"><path d="M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z"/><path d="M14 2v4a2 2 0 0 0 2 2h4"/></svg></span>
        Матеріали
      </a>
    </nav>
    <div class="sidebar-footer">
      <button class="theme-toggle" onclick="toggleTheme()" aria-label="Перемкнути тему">
        <svg id="theme-icon" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 3a6 6 0 0 0 9 9 9 9 0 1 1-9-9Z"/></svg>
        <span id="theme-label">Темна тема</span>
      </button>
    </div>
  </aside>

  <div class="reading-progress">
    <div class="reading-progress-bar"></div>
  </div>

  <main>
    <div class="lecture-nav-top">
      <a href="../lecture.html?id=2" class="back-link">&larr; Назад до лекції 2</a>
      <span class="lecture-badge">Конспект лекції 2</span>
    </div>

    <article class="lecture-content">
      <h1>Математичні основи стеганографії</h1>

      <div class="konspekt-layout">
        <div class="konspekt-text">
          <div class="konspekt-content">


            <!-- Introduction -->
            <section id="intro">
                <h2>Вступ до математичних основ</h2>

                <p>
                    Математичний апарат стеганографії базується на теорії інформації, розробленій Клодом Шенноном у 1948 році. Ця теорія дозволяє формалізувати поняття інформації, невизначеності та надлишковості — ключові концепції для розуміння того, як і чому можливе приховування даних у цифрових носіях.
                </p>

                <div class="info-box">
                    <h4>Чому математика важлива для стеганографії?</h4>
                    <ul>
                        <li><strong>Формалізація безпеки</strong> — можливість доведення невиявлюваності стегосистеми</li>
                        <li><strong>Оцінка ємності</strong> — розрахунок максимальної кількості прихованих даних без виявлення</li>
                        <li><strong>Аналіз компромісів</strong> — кількісний баланс між ємністю, непомітністю та стійкістю</li>
                        <li><strong>Розробка алгоритмів</strong> — теоретично обґрунтовані методи вбудовування з гарантіями</li>
                        <li><strong>Оцінка стегоаналізу</strong> — визначення границь можливостей детектора</li>
                    </ul>
                </div>

                <h3>Історичний контекст</h3>

                <p>
                    Стаття Клода Шеннона "A Mathematical Theory of Communication" (1948) заклала фундамент для всієї сучасної теорії комунікацій. Його подальша робота "Communication Theory of Secrecy Systems" (1949) формалізувала криптографію. У 1998 році Крістіан Кашин (Christian Cachin) застосував ідеї теорії інформації для формалізації безпеки стеганографії у роботі "An Information-Theoretic Model for Steganography", що стало основою сучасного теоретичного підходу.
                </p>

                <p>
                    У 2004 році Філлер, Юдіча і Фрідріх дослідили зв'язок між ефективністю вбудовування та виявлюваністю, а Філлер і Фрідріх (2010) розробили STC (Syndrome-Trellis Codes) — практично оптимальні коди для стеганографії, що мінімізують кількість змін контейнера.
                </p>
            </section>


            <!-- Shannon Theory -->
            <section id="shannon">
                <h2>1. Теорія інформації Шеннона</h2>

                <h3>1.1. Базова модель комунікації</h3>

                <p>
                    За Шенноном, система комунікації складається з п'яти основних елементів:
                </p>

                <ul>
                    <li><strong>Джерело інформації</strong> — генерує повідомлення для передачі</li>
                    <li><strong>Передавач (кодер)</strong> — перетворює повідомлення у сигнал, придатний для каналу</li>
                    <li><strong>Канал зв'язку</strong> — фізичне середовище передачі (можливо, з шумом)</li>
                    <li><strong>Приймач (декодер)</strong> — відновлює повідомлення з отриманого сигналу</li>
                    <li><strong>Адресат</strong> — кінцевий отримувач інформації</li>
                </ul>

                <p>
                    У стеганографії ця модель набуває додаткового виміру: окрім основного каналу (передача зображення, аудіо, тексту), існує <strong>прихований канал</strong>, який використовує надлишковість основного носія для непомітної передачі секретної інформації.
                </p>

                <h3>1.2. Концепція інформації</h3>

                <p>
                    Шеннон визначив <strong>інформацію</strong> як міру зменшення невизначеності. Чим менш передбачувана подія, тим більше інформації несе її спостереження. Власна інформація (self-information) події x визначається як:
                </p>

                <div class="formula-box">
                    I(x) = &minus;log<sub>2</sub> p(x) біт
                </div>

                <p>
                    де p(x) — ймовірність події x. Логарифм при основі 2 дає результат у бітах.
                </p>

                <div class="example-box">
                    <h4>Приклади обчислення інформації</h4>
                    <p><strong>Чесна монета</strong> (p = 0.5):</p>
                    <p>I = &minus;log<sub>2</sub>(0.5) = 1 біт</p>
                    <p><strong>Кидання кубика</strong> (p = 1/6):</p>
                    <p>I = &minus;log<sub>2</sub>(1/6) &asymp; 2.585 біт</p>
                    <p><strong>Певна подія</strong> (p = 1):</p>
                    <p>I = &minus;log<sub>2</sub>(1) = 0 біт — подія не несе інформації!</p>
                    <p><strong>Рідкісна подія</strong> (p = 0.001):</p>
                    <p>I = &minus;log<sub>2</sub>(0.001) &asymp; 9.97 біт — дуже інформативна</p>
                </div>

                <h4>Властивості власної інформації:</h4>
                <ul>
                    <li><strong>Невід'ємність:</strong> I(x) &ge; 0 (оскільки 0 &le; p(x) &le; 1)</li>
                    <li><strong>Монотонність:</strong> менш імовірні події несуть більше інформації</li>
                    <li><strong>Адитивність:</strong> для незалежних подій I(x,y) = I(x) + I(y)</li>
                    <li><strong>Нульова інформація певної події:</strong> якщо p(x) = 1, то I(x) = 0</li>
                </ul>
            </section>


            <!-- Entropy -->
            <section id="entropy">
                <h2>2. Ентропія та її властивості</h2>

                <h3>2.1. Ентропія Шеннона</h3>

                <p>
                    <strong>Ентропія</strong> — середня (очікувана) кількість інформації, що міститься у випадковій величині. Це фундаментальна величина, яка визначає мінімальну середню кількість бітів для кодування символів джерела.
                </p>

                <div class="formula-box">
                    H(X) = &minus;&sum;<sub>x&isin;X</sub> p(x) log<sub>2</sub> p(x) біт
                </div>

                <p>де сума береться по всіх можливих значеннях x із множини X.</p>

                <h4>Властивості ентропії:</h4>
                <ul>
                    <li><strong>Невід'ємність:</strong> H(X) &ge; 0</li>
                    <li><strong>Максимум:</strong> H(X) &le; log<sub>2</sub> |X| — рівність при рівноймовірних подіях</li>
                    <li><strong>Нуль:</strong> H(X) = 0 тільки коли подія детермінована (p = 1 для одного з результатів)</li>
                    <li><strong>Конкавність:</strong> H є угнутою функцією вектора ймовірностей</li>
                </ul>

                <div class="example-box">
                    <h4>Приклад: ентропія бінарного джерела</h4>
                    <p>
                        Для джерела з двома символами (0 та 1), де P(0) = p, P(1) = 1&minus;p:
                    </p>
                    <div class="formula-box">
                        H<sub>b</sub>(p) = &minus;p log<sub>2</sub> p &minus; (1&minus;p) log<sub>2</sub>(1&minus;p)
                    </div>
                    <table class="comparison-table">
                        <tr>
                            <th>p</th>
                            <th>H<sub>b</sub>(p)</th>
                            <th>Інтерпретація</th>
                        </tr>
                        <tr>
                            <td>0.0</td>
                            <td>0.000</td>
                            <td>Повна визначеність (завжди 1)</td>
                        </tr>
                        <tr>
                            <td>0.1</td>
                            <td>0.469</td>
                            <td>Переважно 1, мало невизначеності</td>
                        </tr>
                        <tr>
                            <td>0.3</td>
                            <td>0.881</td>
                            <td>Нерівномірний, але менш передбачуваний</td>
                        </tr>
                        <tr>
                            <td>0.5</td>
                            <td>1.000</td>
                            <td>Максимальна невизначеність</td>
                        </tr>
                    </table>
                    <p>Максимум ентропії досягається при p = 0.5 (чесна монета).</p>
                </div>

                <div class="example-box">
                    <h4>Приклад: ентропія природних мов</h4>
                    <p>
                        Шеннон обчислив, що ентропія англійської мови становить приблизно <strong>2.62 біт на букву</strong>,
                        що значно менше за теоретичних log<sub>2</sub>(26) &asymp; 4.7 біт (якби всі 26 букв були рівноймовірні).
                    </p>
                    <p>
                        Ця різниця (4.7 &minus; 2.62 = 2.08 біт) — <strong>надлишковість</strong>, яку можна експлуатувати для стеганографії та стиснення.
                    </p>
                    <p>
                        Для української мови (33 букви) теоретичний максимум — log<sub>2</sub>(33) &asymp; 5.04 біт/букву, а фактична ентропія ще нижча через більшу морфологічну складність.
                    </p>
                </div>

                <h3>2.2. Спільна ентропія</h3>

                <p>
                    Спільна ентропія вимірює загальну невизначеність пари випадкових величин (X, Y):
                </p>

                <div class="formula-box">
                    H(X,Y) = &minus;&sum;<sub>x,y</sub> p(x,y) log<sub>2</sub> p(x,y)
                </div>

                <h4>Властивості:</h4>
                <ul>
                    <li>H(X,Y) &le; H(X) + H(Y) — рівність тільки при <strong>незалежності</strong> X та Y</li>
                    <li>H(X,Y) &ge; max(H(X), H(Y)) — пара завжди не менш невизначена ніж кожна окремо</li>
                    <li>H(X,Y) = H(X) + H(Y|X) — ланцюгове правило</li>
                </ul>

                <div class="info-box">
                    <h4>Значення для стеганографії</h4>
                    <p>
                        Якщо пікселі зображення X<sub>i</sub> та X<sub>i+1</sub> сильно корельовані (залежні), то H(X<sub>i</sub>, X<sub>i+1</sub>) значно менше за H(X<sub>i</sub>) + H(X<sub>i+1</sub>). Ця різниця — <strong>просторова надлишковість</strong>, яку стеганографія може використовувати для приховування даних.
                    </p>
                </div>

                <h3>2.3. Умовна ентропія</h3>

                <p>
                    Умовна ентропія H(Y|X) — кількість інформації, потрібної для опису Y при відомому X:
                </p>

                <div class="formula-box">
                    H(Y|X) = &minus;&sum;<sub>x,y</sub> p(x,y) log<sub>2</sub> p(y|x) = H(X,Y) &minus; H(X)
                </div>

                <h4>Ключова властивість:</h4>

                <div class="formula-box">
                    H(Y|X) &le; H(Y)
                </div>

                <p>
                    Знання X ніколи не може <em>збільшити</em> невизначеність щодо Y. Рівність досягається тільки коли X та Y незалежні.
                </p>

                <div class="example-box">
                    <h4>Приклад: пікселі зображення</h4>
                    <p>
                        Нехай X — значення поточного пікселя (0&ndash;255), Y — значення сусіднього пікселя.
                    </p>
                    <ul>
                        <li>H(Y) &asymp; 7.5 біт (маргінальна ентропія пікселя)</li>
                        <li>H(Y|X) &asymp; 3.5 біт (умовна ентропія при відомому сусіді)</li>
                    </ul>
                    <p>
                        Різниця 7.5 &minus; 3.5 = 4 біти — це <strong>взаємна інформація</strong> між сусідніми пікселями, яка відображає їх кореляцію.
                    </p>
                </div>

                <h3>2.4. Взаємна інформація</h3>

                <p>
                    Взаємна інформація — кількість інформації, яку одна випадкова величина несе про іншу:
                </p>

                <div class="formula-box">
                    I(X;Y) = H(X) &minus; H(X|Y) = H(Y) &minus; H(Y|X) = H(X) + H(Y) &minus; H(X,Y)
                </div>

                <h4>Властивості:</h4>
                <ul>
                    <li><strong>I(X;Y) &ge; 0</strong> — завжди невід'ємна</li>
                    <li><strong>I(X;Y) = I(Y;X)</strong> — симетрична</li>
                    <li><strong>I(X;Y) = 0</strong> &hArr; X та Y незалежні</li>
                    <li><strong>I(X;X) = H(X)</strong> — максимальна інформація про себе</li>
                </ul>

                <div class="info-box">
                    <h4>Діаграма Венна для інформаційних величин</h4>
                    <p>Зв'язки між усіма величинами найлегше зрозуміти через діаграму:</p>
                    <ul>
                        <li>H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)</li>
                        <li>I(X;Y) = H(X) + H(Y) &minus; H(X,Y)</li>
                        <li>I(X;Y) = H(X) &minus; H(X|Y)</li>
                    </ul>
                    <p>
                        Для стеганографії: I(C;S) між контейнером C та стего-об'єктом S показує, скільки інформації про секретне повідомлення можна отримати зі спостереження стего-об'єкта. Ідеальна стегосистема мінімізує I(C;S&minus;C), де S&minus;C — різниця між стего-об'єктом та контейнером.
                    </p>
                </div>

                <h3>2.5. Крос-ентропія</h3>

                <p>
                    Крос-ентропія H(P,Q) — середня кількість бітів, потрібних для кодування даних з розподілу P, використовуючи код, оптимальний для розподілу Q:
                </p>

                <div class="formula-box">
                    H(P,Q) = &minus;&sum;<sub>x</sub> P(x) log<sub>2</sub> Q(x) = H(P) + D<sub>KL</sub>(P || Q)
                </div>

                <p>
                    Крос-ентропія завжди не менша за ентропію: H(P,Q) &ge; H(P). Різниця — це KL-дивергенція, яка вимірює "штраф" за використання неправильного розподілу.
                </p>

                <div class="warning-box">
                    <h4>Зв'язок зі стегоаналізом</h4>
                    <p>
                        Стегоаналіз на основі машинного навчання часто використовує <strong>крос-ентропію як функцію втрат</strong> для тренування класифікаторів cover/stego. Мінімізація крос-ентропії еквівалентна максимізації правдоподібності правильної класифікації.
                    </p>
                </div>
            </section>


            <!-- Redundancy -->
            <section id="redundancy">
                <h2>3. Надлишковість цифрових носіїв</h2>

                <h3>3.1. Визначення надлишковості</h3>

                <p>
                    <strong>Надлишковість</strong> — різниця між максимально можливою ентропією та фактичною ентропією джерела. Саме надлишковість створює "простір" для приховування даних без помітної зміни носія.
                </p>

                <div class="formula-box">
                    R = H<sub>max</sub> &minus; H<sub>actual</sub> = log<sub>2</sub> |X| &minus; H(X)
                </div>

                <p>Відносна надлишковість (у частках):</p>

                <div class="formula-box">
                    r = 1 &minus; H(X) / log<sub>2</sub> |X|
                </div>

                <div class="example-box">
                    <h4>Надлишковість різних носіїв</h4>
                    <table class="comparison-table">
                        <tr>
                            <th>Носій</th>
                            <th>H<sub>max</sub></th>
                            <th>H<sub>actual</sub></th>
                            <th>Надлишковість</th>
                            <th>r (%)</th>
                        </tr>
                        <tr>
                            <td>8-біт зображення (піксель)</td>
                            <td>8.0 біт</td>
                            <td>~5.5 біт</td>
                            <td>~2.5 біт</td>
                            <td>~31%</td>
                        </tr>
                        <tr>
                            <td>Англійський текст (буква)</td>
                            <td>4.7 біт</td>
                            <td>~2.6 біт</td>
                            <td>~2.1 біт</td>
                            <td>~45%</td>
                        </tr>
                        <tr>
                            <td>16-біт аудіо (семпл)</td>
                            <td>16.0 біт</td>
                            <td>~12.0 біт</td>
                            <td>~4.0 біт</td>
                            <td>~25%</td>
                        </tr>
                        <tr>
                            <td>DCT коефіцієнти JPEG</td>
                            <td>~10 біт</td>
                            <td>~4.5 біт</td>
                            <td>~5.5 біт</td>
                            <td>~55%</td>
                        </tr>
                    </table>
                </div>

                <h3>3.2. Типи надлишковості</h3>

                <table class="comparison-table">
                    <tr>
                        <th>Тип</th>
                        <th>Опис</th>
                        <th>Приклад</th>
                        <th>Використання в стеганографії</th>
                    </tr>
                    <tr>
                        <td><strong>Просторова</strong></td>
                        <td>Кореляція між сусідніми елементами</td>
                        <td>Сусідні пікселі зображення схожі</td>
                        <td>Адаптивне вбудовування у текстурованих ділянках</td>
                    </tr>
                    <tr>
                        <td><strong>Часова</strong></td>
                        <td>Кореляція між послідовними відліками</td>
                        <td>Сусідні семпли аудіо корельовані</td>
                        <td>Аудіо стеганографія, відео між кадрами</td>
                    </tr>
                    <tr>
                        <td><strong>Спектральна</strong></td>
                        <td>Кореляція між частотними компонентами</td>
                        <td>Низькі частоти домінують у природних зображеннях</td>
                        <td>DCT/DWT методи, спектральне маскування</td>
                    </tr>
                    <tr>
                        <td><strong>Перцептивна</strong></td>
                        <td>Інформація, непомітна для органів чуття</td>
                        <td>LSB пікселів, ультразвукові частоти</td>
                        <td>LSB стеганографія, маскування шумом</td>
                    </tr>
                    <tr>
                        <td><strong>Кодова</strong></td>
                        <td>Надлишковість формату кодування</td>
                        <td>Порядок атрибутів HTML, пробіли в коді</td>
                        <td>Форматна стеганографія, текстова стеганографія</td>
                    </tr>
                </table>

                <h3>3.3. Надлишковість зображень: детальний аналіз</h3>

                <p>
                    Природні зображення мають характерні статистичні властивості, що створюють надлишковість на різних рівнях:
                </p>

                <h4>Піксельний рівень:</h4>
                <ul>
                    <li><strong>Кореляція сусідніх пікселів:</strong> типовий коефіцієнт кореляції &rho; &asymp; 0.90&ndash;0.97 для сусідніх пікселів у природних зображеннях</li>
                    <li><strong>Гладкі області:</strong> мають дуже низьку локальну дисперсію (&sigma;&sup2; &lt; 5)</li>
                    <li><strong>Краї:</strong> різкі переходи яскравості займають лише 5&ndash;15% площі зображення</li>
                    <li><strong>Розподіл різниць:</strong> різниці між сусідніми пікселями розподілені приблизно за законом Лапласа з центром у нулі</li>
                </ul>

                <h4>Частотний рівень:</h4>
                <ul>
                    <li><strong>Спектральний спад:</strong> енергія спектру природних зображень спадає як 1/f&sup2;, де f — частота</li>
                    <li><strong>DCT коефіцієнти:</strong> розподілені за узагальненим гаусівським розподілом (GGD) з параметром форми &beta; &asymp; 0.5 (розподіл Лапласа)</li>
                    <li><strong>Квантовані DCT:</strong> після квантування JPEG більшість коефіцієнтів обнуляються (60&ndash;90%)</li>
                </ul>

                <div class="example-box">
                    <h4>Кількісна оцінка надлишковості зображення</h4>
                    <p>
                        8-бітне зображення теоретично може нести до 8 біт/піксель. Через кореляцію фактична ентропія природних зображень:
                    </p>
                    <ul>
                        <li><strong>Маргінальна ентропія:</strong> H(X<sub>i</sub>) &asymp; 7.0&ndash;7.5 біт/піксель</li>
                        <li><strong>Ентропія першого порядку різниць:</strong> H(X<sub>i</sub> &minus; X<sub>i-1</sub>) &asymp; 3.5&ndash;5.0 біт</li>
                        <li><strong>Оцінка ентропійного кодера (PNG):</strong> ~4.0&ndash;6.0 біт/піксель</li>
                    </ul>
                    <p><strong>Надлишковість &asymp; 2&ndash;4 біт/піксель</strong> — потенційний простір для стеганографії. Але це <strong>теоретична</strong> оцінка; безпечна ємність значно менша (див. закон &radic;n).</p>
                </div>

                <h3>3.4. Експлуатація надлишковості</h3>

                <div class="two-columns">
                    <div>
                        <h4>Просторові методи (LSB)</h4>
                        <ul>
                            <li>Використовують перцептивну надлишковість</li>
                            <li>Зміни LSB непомітні для ока (&plusmn;1)</li>
                            <li>Ємність: 1&ndash;3 біт/піксель</li>
                            <li>Легко детектуються статистично</li>
                        </ul>
                    </div>
                    <div>
                        <h4>Частотні методи (DCT/DWT)</h4>
                        <ul>
                            <li>Використовують спектральну надлишковість</li>
                            <li>Вбудовування в DCT/DWT коефіцієнти</li>
                            <li>Більша стійкість до обробки</li>
                            <li>Працюють з JPEG та іншими стиснутими форматами</li>
                        </ul>
                    </div>
                </div>

                <div class="two-columns">
                    <div>
                        <h4>Адаптивні методи</h4>
                        <ul>
                            <li>Використовують <em>варіацію</em> надлишковості</li>
                            <li>Більше змін у текстурах, менше у гладких областях</li>
                            <li>Мінімізують загальне спотворення</li>
                            <li>HUGO, WOW, S-UNIWARD</li>
                        </ul>
                    </div>
                    <div>
                        <h4>Кодові методи</h4>
                        <ul>
                            <li>Використовують кодову надлишковість</li>
                            <li>Зміни у форматі, не у вмісті</li>
                            <li>Текстова стеганографія, HTML</li>
                            <li>Дуже низька ємність</li>
                        </ul>
                    </div>
                </div>
            </section>


            <!-- Statistics -->
            <section id="statistics">
                <h2>4. Статистичні характеристики контейнерів</h2>

                <h3>4.1. Статистики першого порядку</h3>

                <p>
                    Статистики першого порядку залежать лише від окремих значень елементів (пікселів, семплів), без урахування просторових або часових зв'язків:
                </p>

                <ul>
                    <li><strong>Гістограма h(k):</strong> кількість елементів зі значенням k (для зображень k &isin; [0, 255])</li>
                    <li><strong>Середнє:</strong> &mu; = (1/N) &sum; x<sub>i</sub></li>
                    <li><strong>Дисперсія:</strong> &sigma;&sup2; = (1/N) &sum; (x<sub>i</sub> &minus; &mu;)&sup2;</li>
                    <li><strong>Асиметрія (skewness):</strong> &gamma;<sub>1</sub> = E[(X&minus;&mu;)&sup3;] / &sigma;&sup3;</li>
                    <li><strong>Ексцес (kurtosis):</strong> &gamma;<sub>2</sub> = E[(X&minus;&mu;)&sup4;] / &sigma;&sup4; &minus; 3</li>
                </ul>

                <div class="warning-box">
                    <h4>Вплив LSB стеганографії на статистики першого порядку</h4>
                    <p>
                        Просте LSB вбудовування змінює гістограму характерним чином: пари суміжних значень (2k, 2k+1) — так звані <strong>Pairs of Values (PoVs)</strong> — прагнуть до однакових частот. Ця аномалія детектується <strong>Chi-Square (&chi;&sup2;) атакою</strong> Вестфельда.
                    </p>
                    <p>
                        Конкретно: для кожної пари (2k, 2k+1) статистика &chi;&sup2; = &sum; (h(2k) &minus; h(2k+1))&sup2; / (h(2k) + h(2k+1)) наближається до нуля при повному LSB вбудовуванні.
                    </p>
                </div>

                <h3>4.2. Статистики вищих порядків</h3>

                <p>
                    Статистики вищих порядків враховують <strong>залежності між елементами</strong> — саме вони роблять сучасний стегоаналіз таким ефективним:
                </p>

                <h4>Матриці суміжності (co-occurrence matrices):</h4>
                <p>
                    Матриця C<sub>d</sub>(i,j) рахує кількість пар пікселів зі значеннями i та j, розташованих на відстані d:
                </p>
                <div class="formula-box">
                    C<sub>d</sub>(i,j) = |{(x,y) : f(x,y) = i, f(x+d<sub>x</sub>, y+d<sub>y</sub>) = j}|
                </div>
                <p>
                    Для природних зображень ця матриця сконцентрована біля діагоналі (сусідні пікселі схожі). LSB вбудовування розмиває цю концентрацію.
                </p>

                <h4>Різницеві масиви (residual arrays):</h4>
                <p>
                    Обчислення різниць між сусідніми пікселями у різних напрямках. Модель SPAM (Subtractive Pixel Adjacency Matrix) використовує різниці першого порядку:
                </p>
                <div class="formula-box">
                    r<sub>h</sub>(i,j) = f(i, j+1) &minus; f(i, j) — горизонтальна різниця
                    <br>
                    r<sub>v</sub>(i,j) = f(i+1, j) &minus; f(i, j) — вертикальна різниця
                </div>

                <h4>Багаті моделі ознак (Rich Models):</h4>
                <p>
                    SRM (Spatial Rich Model) обчислює тисячі ознак на основі різницевих фільтрів різних порядків та напрямків. Це дозволяє виявляти навіть мінімальні зміни у статистиці зображення.
                </p>

                <table class="comparison-table">
                    <tr>
                        <th>Модель</th>
                        <th>Кількість ознак</th>
                        <th>Рік</th>
                        <th>Принцип</th>
                    </tr>
                    <tr>
                        <td>SPAM</td>
                        <td>686</td>
                        <td>2010</td>
                        <td>Марківські переходи на різницях</td>
                    </tr>
                    <tr>
                        <td>SRM</td>
                        <td>34,671</td>
                        <td>2012</td>
                        <td>Різницеві фільтри + co-occurrence</td>
                    </tr>
                    <tr>
                        <td>maxSRMd2</td>
                        <td>12,753</td>
                        <td>2012</td>
                        <td>Оптимізована версія SRM</td>
                    </tr>
                </table>

                <h3>4.3. Марківські моделі</h3>

                <p>
                    Пікселі зображення не є незалежними однаково розподіленими (i.i.d.) — вони мають сильні просторові кореляції. <strong>Марківська модель</strong> формалізує цю залежність:
                </p>

                <div class="formula-box">
                    P(X<sub>n</sub> | X<sub>n-1</sub>, X<sub>n-2</sub>, ..., X<sub>1</sub>) = P(X<sub>n</sub> | X<sub>n-1</sub>)
                </div>

                <p>
                    Ймовірність поточного значення залежить лише від безпосередньо попереднього — це <strong>марківський ланцюг першого порядку</strong>. Для зображень розглядають ланцюги вищих порядків та двовимірні марківські поля.
                </p>

                <h4>Матриця переходів:</h4>
                <p>
                    Для марківського ланцюга першого порядку на множині {0, ..., 255} матриця переходів T має розмір 256&times;256, де T(i,j) = P(X<sub>n</sub>=j | X<sub>n-1</sub>=i).
                </p>

                <div class="info-box">
                    <h4>Застосування марківських моделей у стеганографії</h4>
                    <ul>
                        <li><strong>Стегоаналіз (SPAM):</strong> порушення марківських переходів — ознака вбудовування. SPAM модель обчислює матрицю переходів для горизонтальних, вертикальних та діагональних різниць.</li>
                        <li><strong>Адаптивна стеганографія:</strong> збереження марківської статистики при вбудовуванні — одна з цілей сучасних методів (MiPOD, MG).</li>
                        <li><strong>Теоретичний аналіз:</strong> марківська модель дозволяє строго довести <strong>закон квадратного кореня</strong> для безпечної ємності.</li>
                    </ul>
                </div>
            </section>


            <!-- Capacity -->
            <section id="capacity">
                <h2>5. Стеганографічна ємність</h2>

                <h3>5.1. Пропускна здатність стегоканалу</h3>

                <p>
                    За аналогією з теоремою Шеннона для каналу зв'язку, <strong>пропускна здатність стегоканалу</strong> — максимальна кількість бітів, яку можна надійно сховати при заданому рівні безпеки:
                </p>

                <div class="formula-box">
                    C<sub>steg</sub> = max<sub>E</sub> {m : D<sub>KL</sub>(P<sub>C</sub> || P<sub>S</sub>) &le; &epsilon;}
                </div>

                <p>
                    де E — алгоритм вбудовування, m — кількість прихованих бітів, &epsilon; — допустимий рівень виявлюваності.
                </p>

                <h3>5.2. Закон квадратного кореня (Square Root Law)</h3>

                <p>
                    Один з найважливіших результатів теоретичної стеганографії, встановлений Філлером, Фрідріхом та Кершнером (2004), а також незалежно Кером та Фонтана (2008):
                </p>

                <div class="important-box">
                    <h4>Закон квадратного кореня</h4>
                    <p>
                        Для стегосистеми, яка вбудовує m бітів у контейнер розміром n елементів, існують константи c<sub>1</sub>, c<sub>2</sub> &gt; 0 такі, що:
                    </p>
                    <div class="formula-box">
                        D<sub>KL</sub>(P<sub>C</sub> || P<sub>S</sub>) &asymp; c<sub>1</sub> &middot; m&sup2; / n
                    </div>
                    <p>
                        Для підтримки &epsilon;-безпеки (D<sub>KL</sub> &le; &epsilon;) потрібно:
                    </p>
                    <div class="formula-box">
                        m &le; c<sub>2</sub> &middot; &radic;(&epsilon; &middot; n) &sim; O(&radic;n)
                    </div>
                    <p><strong>Безпечна ємність пропорційна квадратному кореню з розміру контейнера.</strong></p>
                </div>

                <div class="example-box">
                    <h4>Практичне значення закону &radic;n</h4>
                    <table class="comparison-table">
                        <tr>
                            <th>Контейнер</th>
                            <th>Розмір n</th>
                            <th>&radic;n</th>
                            <th>Безпечна ємність (порядок)</th>
                        </tr>
                        <tr>
                            <td>256&times;256 зобр.</td>
                            <td>65,536</td>
                            <td>256</td>
                            <td>~32 байти</td>
                        </tr>
                        <tr>
                            <td>512&times;512 зобр.</td>
                            <td>262,144</td>
                            <td>512</td>
                            <td>~64 байти</td>
                        </tr>
                        <tr>
                            <td>1024&times;1024 зобр.</td>
                            <td>1,048,576</td>
                            <td>1,024</td>
                            <td>~128 байт</td>
                        </tr>
                        <tr>
                            <td>4K зображення</td>
                            <td>8,294,400</td>
                            <td>2,880</td>
                            <td>~360 байт</td>
                        </tr>
                    </table>
                    <p>
                        <strong>Важливо:</strong> подвоєння розміру контейнера збільшує безпечну ємність лише в &radic;2 &asymp; 1.41 рази, а не вдвічі!
                    </p>
                </div>

                <div class="warning-box">
                    <h4>Обмеження закону &radic;n</h4>
                    <p>
                        Закон &radic;n справедливий для <strong>недосконалих</strong> стегосистем (що не знають точного розподілу контейнерів). Якщо стегосистема <em>точно</em> відтворює розподіл контейнерів, вона може досягти лінійної ємності O(n) — це так звана <strong>досконала стеганографія</strong>, яка є теоретичною, але практично недосяжною.
                    </p>
                </div>

                <h3>5.3. Ефективність вбудовування</h3>

                <p>
                    Ефективність вбудовування e — відношення кількості вбудованих бітів до кількості змін контейнера:
                </p>

                <div class="formula-box">
                    e = m / R<sub>avg</sub>
                </div>

                <p>де m — кількість вбудованих бітів, R<sub>avg</sub> — середня кількість змінених елементів контейнера.</p>

                <table class="comparison-table">
                    <tr>
                        <th>Метод</th>
                        <th>Ефективність e</th>
                        <th>Змін на біт</th>
                        <th>Принцип</th>
                    </tr>
                    <tr>
                        <td>Просте LSB</td>
                        <td>2.0</td>
                        <td>0.50</td>
                        <td>Кожен піксель = 1 біт, 50% змінюються</td>
                    </tr>
                    <tr>
                        <td>LSB Matching</td>
                        <td>2.0</td>
                        <td>0.50</td>
                        <td>&plusmn;1 замість заміни LSB</td>
                    </tr>
                    <tr>
                        <td>Матричне (1,3,2)</td>
                        <td>4.0</td>
                        <td>0.25</td>
                        <td>Код Хеммінга (3,2)</td>
                    </tr>
                    <tr>
                        <td>Матричне (1,7,3)</td>
                        <td>10.5</td>
                        <td>0.095</td>
                        <td>Код Хеммінга (7,3)</td>
                    </tr>
                    <tr>
                        <td>STC коди</td>
                        <td>Близько до H<sup>&minus;1</sup>(&alpha;)/&alpha;</td>
                        <td>Теоретичний оптимум</td>
                        <td>Syndrome-Trellis Codes</td>
                    </tr>
                </table>

                <div class="info-box">
                    <h4>Теоретична межа ефективності</h4>
                    <p>
                        Максимальна теоретична ефективність при відносному payload &alpha; = m/n (частка змінюваних елементів):
                    </p>
                    <div class="formula-box">
                        e<sub>max</sub> = H<sub>b</sub><sup>&minus;1</sup>(&alpha;) / &alpha;
                    </div>
                    <p>
                        де H<sub>b</sub><sup>&minus;1</sup> — зворотна бінарна ентропія. Наприклад, при &alpha; = 0.05 (5% змінених елементів), e<sub>max</sub> &asymp; 5.85 біт на зміну. STC коди досягають ~98% цієї межі.
                    </p>
                </div>
            </section>


            <!-- Security Models -->
            <section id="security">
                <h2>6. Моделі безпеки стеганографії</h2>

                <h3>6.1. Модель Кашина (Cachin, 1998)</h3>

                <p>
                    Крістіан Кашин формалізував безпеку стеганографії як задачу <strong>перевірки статистичних гіпотез</strong>. Противник (стегоаналітик) спостерігає об'єкт X та повинен розрізнити дві гіпотези:
                </p>

                <ul>
                    <li><strong>H<sub>0</sub>:</strong> X ~ P<sub>C</sub> — об'єкт є "чистим" контейнером</li>
                    <li><strong>H<sub>1</sub>:</strong> X ~ P<sub>S</sub> — об'єкт містить приховані дані (стего-об'єкт)</li>
                </ul>

                <p>
                    Безпека стегосистеми визначається тим, наскільки <strong>розрізнювані</strong> ці два розподіли. Чим ближче P<sub>C</sub> до P<sub>S</sub>, тим складніше противнику виявити стеганографію.
                </p>

                <h3>6.2. KL-дивергенція як міра безпеки</h3>

                <div class="formula-box">
                    D<sub>KL</sub>(P<sub>C</sub> || P<sub>S</sub>) = &sum;<sub>x</sub> P<sub>C</sub>(x) log (P<sub>C</sub>(x) / P<sub>S</sub>(x))
                </div>

                <h4>Властивості KL-дивергенції:</h4>
                <ul>
                    <li><strong>Невід'ємність:</strong> D<sub>KL</sub>(P||Q) &ge; 0 (нерівність Гіббса)</li>
                    <li><strong>Рівність нулю:</strong> D<sub>KL</sub>(P||Q) = 0 &hArr; P = Q</li>
                    <li><strong>Несиметричність:</strong> D<sub>KL</sub>(P||Q) &ne; D<sub>KL</sub>(Q||P) — це <em>не</em> метрика!</li>
                    <li><strong>Адитивність:</strong> для незалежних компонентів D<sub>KL</sub> додаються</li>
                </ul>

                <div class="example-box">
                    <h4>Приклад: KL-дивергенція для LSB</h4>
                    <p>Нехай гістограма зображення має значення h(100) = 50, h(101) = 30. Після LSB вбудовування з повним payload обидва стають (50+30)/2 = 40.</p>
                    <p>KL-дивергенція для цієї пари:</p>
                    <p>D<sub>pair</sub> = (50/80) &middot; log(50/40) + (30/80) &middot; log(30/40) = 0.625 &middot; 0.322 + 0.375 &middot; (&minus;0.415) &asymp; 0.046</p>
                    <p>Загальна D<sub>KL</sub> — сума по всіх 128 парах.</p>
                </div>

                <h3>6.3. &epsilon;-безпека</h3>

                <p>
                    Стегосистема є <strong>&epsilon;-безпечною</strong>, якщо:
                </p>

                <div class="formula-box">
                    D<sub>KL</sub>(P<sub>C</sub> || P<sub>S</sub>) &le; &epsilon;
                </div>

                <p>
                    Чим менше &epsilon;, тим вища безпека стегосистеми. На практиці це означає, що оптимальний статистичний детектор (тест відношення правдоподібності за Нейманом&ndash;Пірсоном) має обмежену здатність до виявлення:
                </p>

                <ul>
                    <li><strong>&epsilon; = 0</strong> — досконала безпека (невиявлювана)</li>
                    <li><strong>&epsilon; &lt; 0.01</strong> — дуже висока безпека (практично невиявлювана)</li>
                    <li><strong>&epsilon; &lt; 0.1</strong> — висока безпека</li>
                    <li><strong>&epsilon; &gt; 1</strong> — стегосистема легко виявляється</li>
                </ul>

                <h3>6.4. Досконала безпека</h3>

                <p>
                    Стегосистема є <strong>досконало безпечною</strong> (perfectly secure), якщо:
                </p>

                <div class="formula-box">
                    D<sub>KL</sub>(P<sub>C</sub> || P<sub>S</sub>) = 0 &hArr; P<sub>C</sub> = P<sub>S</sub>
                </div>

                <div class="important-box">
                    <h4>Теорема Кашина</h4>
                    <p>
                        При D<sub>KL</sub>(P<sub>C</sub> || P<sub>S</sub>) = 0 будь-який статистичний тест з рівнем значущості &alpha; має потужність не більше &alpha;. Тобто навіть <strong>оптимальний детектор не може зробити краще, ніж випадкове вгадування</strong>.
                    </p>
                </div>

                <div class="warning-box">
                    <h4>Парадокс досконалої стеганографії</h4>
                    <p>
                        Досконала стеганографія теоретично можлива, але потребує <strong>точного знання розподілу контейнерів</strong> P<sub>C</sub>. Для природних зображень цей розподіл настільки складний, що точно його змоделювати неможливо. Тому на практиці всі стегосистеми мають &epsilon; &gt; 0.
                    </p>
                </div>

                <h3>6.5. Порівняння моделей безпеки</h3>

                <table class="comparison-table">
                    <tr>
                        <th>Аспект</th>
                        <th>Інформаційно-теоретична</th>
                        <th>Обчислювальна</th>
                        <th>Емпірична</th>
                    </tr>
                    <tr>
                        <td><strong>Противник</strong></td>
                        <td>Необмежений</td>
                        <td>Поліноміально обмежений</td>
                        <td>Конкретний класифікатор</td>
                    </tr>
                    <tr>
                        <td><strong>Гарантія</strong></td>
                        <td>Безумовна</td>
                        <td>Умовна (на складність)</td>
                        <td>Для конкретної атаки</td>
                    </tr>
                    <tr>
                        <td><strong>Міра</strong></td>
                        <td>D<sub>KL</sub></td>
                        <td>Advantage</td>
                        <td>P<sub>E</sub>, AUC</td>
                    </tr>
                    <tr>
                        <td><strong>Квантова стійкість</strong></td>
                        <td>Так</td>
                        <td>Потенційно ні</td>
                        <td>Залежить від атаки</td>
                    </tr>
                    <tr>
                        <td><strong>Практичність</strong></td>
                        <td>Теоретична</td>
                        <td>Теоретична</td>
                        <td>Практична</td>
                    </tr>
                </table>
            </section>


            <!-- Divergence -->
            <section id="divergence">
                <h2>7. Міри статистичної відстані</h2>

                <h3>7.1. KL-дивергенція (відносна ентропія)</h3>

                <div class="formula-box">
                    D<sub>KL</sub>(P || Q) = &sum;<sub>x</sub> P(x) log (P(x) / Q(x))
                </div>

                <p>
                    KL-дивергенція — основна міра безпеки в моделі Кашина. Має інтуїтивну інтерпретацію: середній "сюрприз" від спостереження даних з розподілу P, коли очікувався розподіл Q.
                </p>

                <h4>Властивості:</h4>
                <ul>
                    <li><strong>Невід'ємність:</strong> D<sub>KL</sub>(P||Q) &ge; 0 (нерівність Гіббса)</li>
                    <li><strong>Рівність нулю:</strong> тільки якщо P = Q</li>
                    <li><strong>Несиметричність:</strong> D<sub>KL</sub>(P||Q) &ne; D<sub>KL</sub>(Q||P)</li>
                    <li><strong>Не задовольняє нерівність трикутника</strong> — це не метрика</li>
                    <li><strong>Адитивність:</strong> для продуктових розподілів D<sub>KL</sub>(P<sub>1</sub>&times;P<sub>2</sub> || Q<sub>1</sub>&times;Q<sub>2</sub>) = D<sub>KL</sub>(P<sub>1</sub>||Q<sub>1</sub>) + D<sub>KL</sub>(P<sub>2</sub>||Q<sub>2</sub>)</li>
                </ul>

                <h3>7.2. Симетрична KL-дивергенція (дивергенція Джеффріса)</h3>

                <div class="formula-box">
                    D<sub>J</sub>(P, Q) = D<sub>KL</sub>(P || Q) + D<sub>KL</sub>(Q || P)
                </div>

                <p>
                    Симетрична версія KL-дивергенції. Використовується деякими авторами як альтернативна міра безпеки.
                </p>

                <h3>7.3. Повна варіаційна відстань (Total Variation)</h3>

                <div class="formula-box">
                    d<sub>TV</sub>(P, Q) = (1/2) &sum;<sub>x</sub> |P(x) &minus; Q(x)|
                </div>

                <h4>Властивості:</h4>
                <ul>
                    <li>Діапазон: [0, 1]</li>
                    <li>Симетрична: d<sub>TV</sub>(P,Q) = d<sub>TV</sub>(Q,P)</li>
                    <li>Є метрикою (задовольняє нерівність трикутника)</li>
                    <li><strong>Оперативне значення:</strong> d<sub>TV</sub>(P,Q) = max<sub>A</sub> |P(A) &minus; Q(A)| — максимальна різниця ймовірностей будь-якої події</li>
                </ul>

                <div class="info-box">
                    <h4>Зв'язок з ймовірністю помилки детектора</h4>
                    <p>
                        Для оптимального детектора (тест відношення правдоподібності) мінімальна ймовірність помилки:
                    </p>
                    <div class="formula-box">
                        P<sub>E</sub> = (1 &minus; d<sub>TV</sub>(P<sub>C</sub>, P<sub>S</sub>)) / 2
                    </div>
                    <p>
                        При d<sub>TV</sub> = 0 &rarr; P<sub>E</sub> = 0.5 (випадкове вгадування). При d<sub>TV</sub> = 1 &rarr; P<sub>E</sub> = 0 (ідеальне виявлення).
                    </p>
                </div>

                <h3>7.4. Нерівність Пінскера</h3>

                <p>
                    Фундаментальна нерівність, що зв'язує KL-дивергенцію з повною варіаційною відстанню:
                </p>

                <div class="formula-box">
                    d<sub>TV</sub>(P, Q) &le; &radic;((1/2) &middot; D<sub>KL</sub>(P || Q))
                </div>

                <div class="info-box">
                    <h4>Практичне значення для стеганографії</h4>
                    <p>
                        Обмежуючи KL-дивергенцію (&epsilon;-безпека), ми автоматично обмежуємо повну варіаційну відстань:
                    </p>
                    <div class="formula-box">
                        d<sub>TV</sub>(P<sub>C</sub>, P<sub>S</sub>) &le; &radic;(&epsilon;/2)
                    </div>
                    <p>
                        Це, у свою чергу, обмежує ймовірність помилки будь-якого детектора:
                    </p>
                    <div class="formula-box">
                        P<sub>E</sub> &ge; (1 &minus; &radic;(&epsilon;/2)) / 2
                    </div>
                    <p>
                        Наприклад, при &epsilon; = 0.01: d<sub>TV</sub> &le; 0.071, P<sub>E</sub> &ge; 0.465 — детектор майже не краще за монету.
                    </p>
                </div>

                <h3>7.5. Інші міри відстані</h3>

                <table class="comparison-table">
                    <tr>
                        <th>Міра</th>
                        <th>Формула</th>
                        <th>Діапазон</th>
                        <th>Метрика?</th>
                    </tr>
                    <tr>
                        <td>Відстань Хеллінгера</td>
                        <td>H&sup2;(P,Q) = (1/2) &sum; (&radic;P(x) &minus; &radic;Q(x))&sup2;</td>
                        <td>[0, 1]</td>
                        <td>Так</td>
                    </tr>
                    <tr>
                        <td>&chi;&sup2;-дивергенція</td>
                        <td>&chi;&sup2;(P,Q) = &sum; (P(x)&minus;Q(x))&sup2;/Q(x)</td>
                        <td>[0, &infin;)</td>
                        <td>Ні</td>
                    </tr>
                    <tr>
                        <td>Дивергенція Реньї</td>
                        <td>D<sub>&alpha;</sub>(P||Q) = log(&sum; P(x)<sup>&alpha;</sup> Q(x)<sup>1&minus;&alpha;</sup>) / (&alpha;&minus;1)</td>
                        <td>[0, &infin;)</td>
                        <td>Ні</td>
                    </tr>
                </table>
            </section>


            <!-- Conclusion -->
            <section id="conclusion">
                <h2>Висновки та ключові формули</h2>

                <h3>Таблиця ключових формул</h3>

                <table class="comparison-table">
                    <tr>
                        <th>Поняття</th>
                        <th>Формула</th>
                        <th>Значення для стеганографії</th>
                    </tr>
                    <tr>
                        <td>Ентропія Шеннона</td>
                        <td>H(X) = &minus;&sum; p(x) log<sub>2</sub> p(x)</td>
                        <td>Визначає надлишковість контейнера</td>
                    </tr>
                    <tr>
                        <td>Умовна ентропія</td>
                        <td>H(Y|X) = H(X,Y) &minus; H(X)</td>
                        <td>Оцінює залежність між елементами</td>
                    </tr>
                    <tr>
                        <td>Взаємна інформація</td>
                        <td>I(X;Y) = H(X) + H(Y) &minus; H(X,Y)</td>
                        <td>Вимірює кореляцію (надлишковість)</td>
                    </tr>
                    <tr>
                        <td>KL-дивергенція</td>
                        <td>D<sub>KL</sub>(P||Q) = &sum; P(x) log(P(x)/Q(x))</td>
                        <td>Міра виявлюваності стегосистеми</td>
                    </tr>
                    <tr>
                        <td>&epsilon;-безпека</td>
                        <td>D<sub>KL</sub>(P<sub>C</sub>||P<sub>S</sub>) &le; &epsilon;</td>
                        <td>Кількісна гарантія безпеки</td>
                    </tr>
                    <tr>
                        <td>Нерівність Пінскера</td>
                        <td>d<sub>TV</sub> &le; &radic;(D<sub>KL</sub>/2)</td>
                        <td>Зв'язок із ймовірністю виявлення</td>
                    </tr>
                    <tr>
                        <td>Закон &radic;n</td>
                        <td>m &sim; O(&radic;n)</td>
                        <td>Межа безпечної ємності</td>
                    </tr>
                    <tr>
                        <td>Ефективність</td>
                        <td>e = m / R<sub>avg</sub></td>
                        <td>Бітів на зміну контейнера</td>
                    </tr>
                </table>

                <h3>Ключові висновки</h3>

                <div class="info-box">
                    <ul>
                        <li><strong>Теорія інформації Шеннона</strong> — математичний фундамент стеганографії, що дозволяє формалізувати ємність, безпеку та ефективність</li>
                        <li><strong>Ентропія</strong> визначає невизначеність джерела та границі стиснення; надлишковість = H<sub>max</sub> &minus; H</li>
                        <li><strong>Надлишковість</strong> цифрових носіїв (просторова, часова, спектральна, перцептивна) створює простір для приховування</li>
                        <li><strong>Статистики вищих порядків</strong> (co-occurrence, марківські моделі, SRM) — основа сучасного стегоаналізу</li>
                        <li><strong>Закон &radic;n</strong> обмежує безпечну ємність: m &sim; O(&radic;n), подвоєння контейнера дає лише &times;1.41 ємності</li>
                        <li><strong>Модель Кашина</strong> формалізує безпеку через KL-дивергенцію між P<sub>C</sub> та P<sub>S</sub></li>
                        <li><strong>Досконала стеганографія</strong> (D<sub>KL</sub> = 0) теоретично можлива, але потребує точної моделі контейнера</li>
                        <li><strong>Нерівність Пінскера</strong> зв'язує &epsilon;-безпеку з ймовірністю помилки детектора</li>
                    </ul>
                </div>

                <div class="info-box">
                    <h4>Що далі?</h4>
                    <p>
                        У наступній лекції розглянемо <strong>принципи побудови стегосистем</strong> —
                        архітектуру, функціональні та нефункціональні вимоги, методологію проектування та тестування стеганографічних систем на основі розглянутої математичної бази.
                    </p>
                </div>
            </section>


          </div>
        </div>
      <aside class="konspekt-sidebar">
        <div class="section-nav">
          <h4>Зміст</h4>
          <ul>
          <li><a href="#intro">Вступ</a></li>
          <li><a href="#shannon">1. Теорія Шеннона</a></li>
          <li><a href="#entropy">2. Ентропія</a></li>
          <li><a href="#redundancy">3. Надлишковість</a></li>
          <li><a href="#statistics">4. Статистичні моделі</a></li>
          <li><a href="#capacity">5. Ємність</a></li>
          <li><a href="#security">6. Моделі безпеки</a></li>
          <li><a href="#divergence">7. Міри відстані</a></li>
          <li><a href="#conclusion">Висновки</a></li>
          </ul>
        </div>
      </aside>
      </div>
    </article>

    <div class="lecture-nav-bottom">
      <a href="../lecture.html?id=2" class="nav-btn">&larr; До лекції</a>
      <a href="../index.html" class="nav-btn">На головну &rarr;</a>
    </div>

    <footer></footer>
  </main>

  <button class="back-to-top" onclick="window.scrollTo({top: 0, behavior: 'smooth'})">&#8593;</button>

  <script src="../js/main.js"></script>
  <script>
    // Reading Progress Bar
    window.addEventListener('scroll', () => {
      const docHeight = document.documentElement.scrollHeight - window.innerHeight;
      const scrolled = (window.scrollY / docHeight) * 100;
      document.querySelector('.reading-progress-bar').style.width = scrolled + '%';
      const backToTop = document.querySelector('.back-to-top');
      if (window.scrollY > 300) { backToTop.classList.add('show'); }
      else { backToTop.classList.remove('show'); }
    });

    // Active section nav link
    const sections = document.querySelectorAll('section[id]');
    const navLinks = document.querySelectorAll('.section-nav a');
    if (navLinks.length > 0) {
      window.addEventListener('scroll', () => {
        let current = '';
        sections.forEach(section => {
          if (scrollY >= section.offsetTop - 200) {
            current = section.getAttribute('id');
          }
        });
        navLinks.forEach(link => {
          link.classList.remove('active');
          if (link.getAttribute('href') === '#' + current) {
            link.classList.add('active');
          }
        });
      });
    }
  </script>
</body>
</html>
